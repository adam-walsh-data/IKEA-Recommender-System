{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients before backprop\n",
      "Derivative of c with respect to a = None\n",
      "Derivative of c with respect to b = None\n",
      "\n",
      "Gradients after backprop\n",
      "Derivative of c with respect to a = 20.0\n",
      "Derivative of c with respect to b = 15.0\n",
      "\n",
      "Gradients after zero_grad()\n",
      "Derivative of c with respect to a = 0.0\n",
      "Print a grad again = 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "  \n",
    "# Initializing input tensors\n",
    "a = torch.tensor(15.0, requires_grad=True)\n",
    "b = torch.tensor(20.0, requires_grad=True)\n",
    "  \n",
    "# Computing the output\n",
    "c = a * b\n",
    "\n",
    "# Gradients before backward()\n",
    "print('Gradients before backprop')\n",
    "print(f'Derivative of c with respect to a = {a.grad}')\n",
    "print(f'Derivative of c with respect to b = {b.grad}')\n",
    "\n",
    "# Computing the gradients\n",
    "c.backward()\n",
    "\n",
    "# Collecting the output gradient of the\n",
    "# output with respect to the input 'a'\n",
    "derivative_out_a = a.grad\n",
    "  \n",
    "# Collecting the output gradient of the\n",
    "# output with respect to the input 'b'\n",
    "derivative_out_b = b.grad\n",
    "  \n",
    "# Displaying the outputs\n",
    "print('\\nGradients after backprop')\n",
    "print(f'Derivative of c with respect to a = {derivative_out_a}')\n",
    "print(f'Derivative of c with respect to b = {derivative_out_b}')\n",
    "\n",
    "# zero_grad()\n",
    "# torch.optim.Optimizer.zero_grad() only works when optimizer is involved\n",
    "print('\\nGradients after zero_grad()')\n",
    "print(f'Derivative of c with respect to a = {a.grad.zero_()}')\n",
    "print(f'Print a grad again = {a.grad}') # Actually set to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run backwards again\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# You cannot run backwards again on c since dynamic graph is \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# only for one computation, then tensors are freed, new computation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# That is why we need the forward pass first which creates the \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# graph each time again for one batch of data. (input to Linear\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# for example is (*,Hidden_size) so for batch)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRun backwards again\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m c\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mGradients after second backprop\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDerivative of c with respect to a = \u001b[39m\u001b[39m{\u001b[39;00mderivative_out_a\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MasterThesis/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MasterThesis/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# You cannot run backwards again on c since dynamic graph is \n",
    "# only for one computation, then tensors are freed, new computation\n",
    "# That is why we need the forward pass first which creates the \n",
    "# graph each time again for one batch of data. (input to Linear\n",
    "# for example is (*,Hidden_size) so for batch)\n",
    "\n",
    "print(\"Run backwards again\")\n",
    "c.backward()\n",
    "print('\\nGradients after second backprop')\n",
    "print(f'Derivative of c with respect to a = {derivative_out_a}')\n",
    "print(f'Derivative of c with respect to b = {derivative_out_b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run backwards again\n",
      "\n",
      "Gradients after second backprop\n",
      "Derivative of c with respect to a = 20.0\n",
      "Derivative of c with respect to b = 30.0\n"
     ]
    }
   ],
   "source": [
    "# Here we can see that the gradient for b was not reset to 0\n",
    "# so the new gradient is old + new = 2*15\n",
    "# Gradient is always connected to tensor\n",
    "\n",
    "print(\"Run backwards again\")\n",
    "c = a*b\n",
    "c.backward()\n",
    "print('\\nGradients after second backprop')\n",
    "print(f'Derivative of c with respect to a = {derivative_out_a}')\n",
    "print(f'Derivative of c with respect to b = {derivative_out_b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_conn = torch.nn.Linear(in_features=2, out_features=5)\n",
    "parameters = list(fully_conn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1237,  0.6240],\n",
      "        [ 0.3935,  0.6649],\n",
      "        [-0.5858,  0.4895],\n",
      "        [ 0.3314, -0.3378],\n",
      "        [-0.0191,  0.6745]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.3232, -0.1731, -0.2991, -0.1396,  0.1020], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First: W with 5x2\n",
    "print(parameters[0])\n",
    "\n",
    "# Second: b with 5x1\n",
    "parameters[1]\n",
    "\n",
    "# Both have set requires grad to ture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,34], dtype=torch.float32, requires_grad=True)\n",
    "print(a.grad)\n",
    "\n",
    "# Gradient is set to None by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., grad_fn=<DotBackward0>)\n",
      "tensor(9., grad_fn=<DotBackward0>)\n",
      "tensor(9., grad_fn=<DotBackward0>)\n",
      "tensor([1., 2., 3.])\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/ccqkblhx43bf8fpz7y_2_kj80000gn/T/ipykernel_56425/3508248262.py:10: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525849783/work/aten/src/ATen/native/TensorShape.cpp:3281.)\n",
      "  print(x.T @ w)\n"
     ]
    }
   ],
   "source": [
    "# In a dataloader the tensors for inputs/labels are created but it is \n",
    "# set to FALSE as default, so there will be no gradient computations for\n",
    "# the input data (this would harm the efficiency)\n",
    "x = torch.tensor([1,2,3], dtype=torch.float32)\n",
    "w = torch.tensor([1,1,2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# These computations do nothing! Only if assigned to new tensor\n",
    "print(x @ w)\n",
    "print(torch.matmul(x,w))\n",
    "print(x.T @ w)\n",
    "\n",
    "y = x @ w\n",
    "y.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "# Try out the with no grad:\n",
    "# First case: The gradient of z and y wrt w is added \n",
    "x = torch.tensor([1,2,3], dtype=torch.float32)\n",
    "w = torch.tensor([1,1,2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y = x @ w\n",
    "z = x @ w\n",
    "y.backward()\n",
    "z.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Second case: Gradient comps for z are turned off\n",
    "x = torch.tensor([1,2,3], dtype=torch.float32)\n",
    "w = torch.tensor([1,1,2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y = x @ w\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = x @ w\n",
    "\n",
    "y.backward()\n",
    "# z.backward()\n",
    "\n",
    "print(w.grad)\n",
    "\n",
    "# If we do not execute z.backward() it will change nothing in behaviour but under the hood\n",
    "# there will be another (part) of the computational graph that tracks gradients for z! \n",
    "# Even though it might not be executed but the computaitons are saved etc. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When does torch.no_grad() makes sense? \n",
    "- for evaluation for sure, I think memory allocation is already done when computation is done together with the graph (all instructions ho wto compute grad), the execution of it, so the actual computation is triggered when .backward() is called. no_grad will allow for bigger batch sizes in eval phase\n",
    "- in example above, the efficiency was kind of similar, but what happens if you have some weights that are fixed for you. Even if you do not update them, computing the loss will always trigger the gradient computations for them as well! So you should always use no_grad if possible.  **But: If it is an intermediate result you actually need for the gradient comps** , e.g. you need dlayer/dw and dlayer/dlayer_before_output - each weight working in the same direction. The effects (we are talking about bs=1) will all be in the same direction! w1 higher = w2 higher, w2higher = loss smaller so both at the same time = loss down (if the step is reasonably small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 5.0.0 (20220707.1540)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"228pt\" height=\"271pt\"\n",
       " viewBox=\"0.00 0.00 228.00 271.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 267)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-267 224,-267 224,4 -4,4\"/>\n",
       "<!-- 5127667120 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>5127667120</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"130.5,-31 76.5,-31 76.5,0 130.5,0 130.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"103.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 5127990624 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5127990624</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"148,-86 59,-86 59,-67 148,-67 148,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"103.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 5127990624&#45;&gt;5127667120 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>5127990624&#45;&gt;5127667120</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M103.5,-66.79C103.5,-60.07 103.5,-50.4 103.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"107,-41.19 103.5,-31.19 100,-41.19 107,-41.19\"/>\n",
       "</g>\n",
       "<!-- 5127990528 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>5127990528</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"202,-141 113,-141 113,-122 202,-122 202,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"157.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">DotBackward0</text>\n",
       "</g>\n",
       "<!-- 5127990528&#45;&gt;5127990624 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>5127990528&#45;&gt;5127990624</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M148.58,-121.75C140.72,-114.03 129.07,-102.6 119.58,-93.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"121.84,-90.6 112.25,-86.09 116.94,-95.59 121.84,-90.6\"/>\n",
       "</g>\n",
       "<!-- 5127802112 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>5127802112</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-196 0,-196 0,-177 101,-177 101,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 5127802112&#45;&gt;5127990528 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5127802112&#45;&gt;5127990528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M67.69,-176.98C84.75,-168.54 111.13,-155.47 130.9,-145.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.59,-148.75 140,-141.17 129.48,-142.47 132.59,-148.75\"/>\n",
       "</g>\n",
       "<!-- 5127990720 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>5127990720</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-141 6,-141 6,-122 95,-122 95,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 5127802112&#45;&gt;5127990720 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5127802112&#45;&gt;5127990720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.75C50.5,-169.8 50.5,-159.85 50.5,-151.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.09 50.5,-141.09 47,-151.09 54,-151.09\"/>\n",
       "</g>\n",
       "<!-- 5127666960 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5127666960</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-263 23.5,-263 23.5,-232 77.5,-232 77.5,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 5127666960&#45;&gt;5127802112 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5127666960&#45;&gt;5127802112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-231.92C50.5,-224.22 50.5,-214.69 50.5,-206.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-206.25 50.5,-196.25 47,-206.25 54,-206.25\"/>\n",
       "</g>\n",
       "<!-- 5127802160 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5127802160</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"220,-196 119,-196 119,-177 220,-177 220,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 5127802160&#45;&gt;5127990528 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5127802160&#45;&gt;5127990528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.52,-176.75C165.93,-169.72 163.64,-159.62 161.65,-150.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.07,-150.07 159.44,-141.09 158.24,-151.62 165.07,-150.07\"/>\n",
       "</g>\n",
       "<!-- 5127716032 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5127716032</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"196.5,-263 142.5,-263 142.5,-232 196.5,-232 196.5,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 5127716032&#45;&gt;5127802160 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5127716032&#45;&gt;5127802160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.5,-231.92C169.5,-224.22 169.5,-214.69 169.5,-206.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"173,-206.25 169.5,-196.25 166,-206.25 173,-206.25\"/>\n",
       "</g>\n",
       "<!-- 5127990720&#45;&gt;5127990624 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5127990720&#45;&gt;5127990624</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.25,-121.75C66.97,-114.03 78.4,-102.6 87.72,-93.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.31,-95.64 94.91,-86.09 85.36,-90.69 90.31,-95.64\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x131a6ed30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchviz \n",
    "\n",
    "x = torch.tensor([1,2,3], dtype=torch.float32, requires_grad=True)\n",
    "w = torch.tensor([1,1,2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y = x @ w\n",
    "z = x**2\n",
    "res = y + z\n",
    "# torchviz.make_dot(y, show_attrs=True, show_saved=True)\n",
    "torchviz.make_dot(res)\n",
    "\n",
    "# Makes sense since x is influencing y and z!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 5.0.0 (20220707.1540)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"204pt\" height=\"271pt\"\n",
       " viewBox=\"0.00 0.00 204.00 271.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 267)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-267 200,-267 200,4 -4,4\"/>\n",
       "<!-- 4561790432 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>4561790432</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"124.5,-31 70.5,-31 70.5,0 124.5,0 124.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"97.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 5128045280 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5128045280</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"142,-86 53,-86 53,-67 142,-67 142,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"97.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 5128045280&#45;&gt;4561790432 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5128045280&#45;&gt;4561790432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M97.5,-66.79C97.5,-60.07 97.5,-50.4 97.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"101,-41.19 97.5,-31.19 94,-41.19 101,-41.19\"/>\n",
       "</g>\n",
       "<!-- 5128045232 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>5128045232</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"89,-141 0,-141 0,-122 89,-122 89,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"44.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">DotBackward0</text>\n",
       "</g>\n",
       "<!-- 5128045232&#45;&gt;5128045280 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>5128045232&#45;&gt;5128045280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.25,-121.75C60.97,-114.03 72.4,-102.6 81.72,-93.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.31,-95.64 88.91,-86.09 79.36,-90.69 84.31,-95.64\"/>\n",
       "</g>\n",
       "<!-- 5128045088 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>5128045088</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"148,-196 47,-196 47,-177 148,-177 148,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"97.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 5128045088&#45;&gt;5128045232 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5128045088&#45;&gt;5128045232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M88.75,-176.75C81.03,-169.03 69.6,-157.6 60.28,-148.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.64,-145.69 53.09,-141.09 57.69,-150.64 62.64,-145.69\"/>\n",
       "</g>\n",
       "<!-- 5128045376 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5128045376</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"196,-141 107,-141 107,-122 196,-122 196,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">DotBackward0</text>\n",
       "</g>\n",
       "<!-- 5128045088&#45;&gt;5128045376 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5128045088&#45;&gt;5128045376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.42,-176.75C114.28,-169.03 125.93,-157.6 135.42,-148.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.06,-150.59 142.75,-141.09 133.16,-145.6 138.06,-150.59\"/>\n",
       "</g>\n",
       "<!-- 5127998656 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5127998656</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"124.5,-263 70.5,-263 70.5,-232 124.5,-232 124.5,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"97.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 5127998656&#45;&gt;5128045088 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5127998656&#45;&gt;5128045088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M97.5,-231.92C97.5,-224.22 97.5,-214.69 97.5,-206.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"101,-206.25 97.5,-196.25 94,-206.25 101,-206.25\"/>\n",
       "</g>\n",
       "<!-- 5128045376&#45;&gt;5128045280 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5128045376&#45;&gt;5128045280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M142.58,-121.75C134.72,-114.03 123.07,-102.6 113.58,-93.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.84,-90.6 106.25,-86.09 110.94,-95.59 115.84,-90.6\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x131a7c310>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3], dtype=torch.float32, requires_grad=False)\n",
    "w = torch.tensor([1,1,2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y = x @ w\n",
    "z = x @ w\n",
    "res = y + z\n",
    "torchviz.make_dot(res)\n",
    "\n",
    "# Lefs represent the trainable parameters, here x is not trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "tensor([13., 26., 39.])\n"
     ]
    }
   ],
   "source": [
    "y = x @ w\n",
    "z = x @ w\n",
    "res = y + z\n",
    "res.backward()  # This works since the comp graph is created from operations! \n",
    "print(w.grad)\n",
    "\n",
    "y = 10*x@w\n",
    "z = x @ w\n",
    "res = y + z\n",
    "res.backward()\n",
    "print(w.grad)\n",
    "\n",
    "# We can alter the formulation and still backward it to acqiure gradient, new graph is built!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AccumulateGrad is accumulating the grads for this leaf node (weights) over the batch (num times it is called before zero_gradding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 5.0.0 (20220707.1540)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"109pt\" height=\"271pt\"\n",
       " viewBox=\"0.00 0.00 109.00 271.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 267)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-267 105,-267 105,4 -4,4\"/>\n",
       "<!-- 5127667360 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>5127667360</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"77.5,-31 23.5,-31 23.5,0 77.5,0 77.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 5128046192 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5128046192</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-86 6,-86 6,-67 95,-67 95,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MaxBackward1</text>\n",
       "</g>\n",
       "<!-- 5128046192&#45;&gt;5127667360 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5128046192&#45;&gt;5127667360</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-66.79C50.5,-60.07 50.5,-50.4 50.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-41.19 50.5,-31.19 47,-41.19 54,-41.19\"/>\n",
       "</g>\n",
       "<!-- 5128044944 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>5128044944</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-141 6,-141 6,-122 95,-122 95,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 5128044944&#45;&gt;5128046192 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>5128044944&#45;&gt;5128046192</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-121.75C50.5,-114.8 50.5,-104.85 50.5,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-96.09 50.5,-86.09 47,-96.09 54,-96.09\"/>\n",
       "</g>\n",
       "<!-- 5128044608 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>5128044608</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-196 0,-196 0,-177 101,-177 101,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 5128044608&#45;&gt;5128044944 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5128044608&#45;&gt;5128044944</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.75C50.5,-169.8 50.5,-159.85 50.5,-151.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.09 50.5,-141.09 47,-151.09 54,-151.09\"/>\n",
       "</g>\n",
       "<!-- 5127714352 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5127714352</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-263 23.5,-263 23.5,-232 77.5,-232 77.5,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 5127714352&#45;&gt;5128044608 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5127714352&#45;&gt;5128044608</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-231.92C50.5,-224.22 50.5,-214.69 50.5,-206.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-206.25 50.5,-196.25 47,-206.25 54,-206.25\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x131a7c070>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2.1,3], dtype=torch.float32, requires_grad=False)\n",
    "w = torch.tensor([1,3,2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "z = torch.max(x*w)\n",
    "z.backward()\n",
    "\n",
    "torchviz.make_dot(z)\n",
    "\n",
    "# Makes sense since x is influencing y and z!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 2.1000, 0.0000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad\n",
    "\n",
    "# Gradient for operation just ignores all other inputs and just lets the gradient flow \n",
    "# backwards from this hihgest input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happens with gradient during indexing\n",
    "x = torch.tensor([1,2,3], dtype=torch.float32, requires_grad=False)\n",
    "w = torch.tensor([1,5,7], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "z = w*x\n",
    "result = z[1]\n",
    "\n",
    "print(result.requires_grad)\n",
    "\n",
    "result.backward()\n",
    "w.grad\n",
    "\n",
    "# Identity function for one element! So gradient flows only back from this element (basically 0*x_0*w_0!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  7., 10.], grad_fn=<AddBackward0>)\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happens with gradient during indexing\n",
    "x = torch.tensor([1,2,3], dtype=torch.float32, requires_grad=False)\n",
    "w = torch.tensor([1,5,7], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "z = x + w \n",
    "print(z)\n",
    "print(z.requires_grad)\n",
    "\n",
    "res = z.gather(0, index=torch.tensor(1))\n",
    "res.backward()\n",
    "w.grad\n",
    "\n",
    "# The gradient only flows from the second element in z since \n",
    "# it is gathered, rest is not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m b \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m process(a,b)\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      4\u001b[0m c \u001b[39m=\u001b[39m a\u001b[39m+\u001b[39mb\n\u001b[1;32m      5\u001b[0m \u001b[39m# c = c.to(\"cpu\")\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m c \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m      7\u001b[0m \u001b[39mreturn\u001b[39;00m c\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# What happens if on GPU and then numpy?\n",
    "\n",
    "def process(a, b):\n",
    "    c = a+b\n",
    "    # c = c.to(\"cpu\")\n",
    "    c = c.numpy()\n",
    "    return c.sum()\n",
    "\n",
    "a = torch.tensor([15, 20, 30])\n",
    "b = torch.tensor([150, 200, 300])\n",
    "\n",
    "a = a.to(\"mps\")\n",
    "b = b.to(\"mps\")\n",
    "\n",
    "process(a,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with 3D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1., 10.,  1.],\n",
      "        [ 2., 20.,  2.],\n",
      "        [ 3., 30.,  3.],\n",
      "        [ 4., 40.,  4.],\n",
      "        [ 5., 50.,  5.]], grad_fn=<StackBackward0>)\n",
      "True\n",
      "None\n",
      "tensor([3., 3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "a = torch.Tensor([1,2,3,4,5])\n",
    "b = torch.Tensor([10,20,30,40,50]).requires_grad_()\n",
    "c = torch.Tensor([1,2,3,4,5])\n",
    "\n",
    "stack = torch.stack([a,b,c],dim=1)\n",
    "print(stack)\n",
    "print(b.requires_grad)\n",
    "\n",
    "res = stack @ torch.Tensor([1,3,1])\n",
    "res = res.sum()\n",
    "res.backward()\n",
    "\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[1,2,3,4,5], [-1,-2,-3,-4,-5]])\n",
    "b = torch.Tensor([[10,20,30,40,50], [0.1,0.2,0.3,0.4,0.5]]).requires_grad_()\n",
    "c = torch.Tensor([[100,200,300,400,500], [11,21,31,41,51]])\n",
    "\n",
    "res = torch.stack([a,b,c], dim=1)\n",
    "res.shape\n",
    "# (2,3,5) - (batch_size, q-heads, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets gather the right actions\n",
    "actions = torch.tensor([1,0])  # Action 1 for batch 1, 0 for batch 2\n",
    "actions = actions.unsqueeze(1).unsqueeze(2)\n",
    "torch.gather(res, dim=2, index=actions).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index must be of size that we want.\n",
    "# We want (batch_size, num_heads, 1) so gather along 2\n",
    "# Index needs to be of same size so (2, 3, 1)\n",
    "# https://medium.com/@mbednarski/understanding-indexing-with-pytorch-gather-33717a84ebc4\n",
    "\n",
    "actions = torch.tensor([1,0]).unsqueeze(1)\n",
    "idx = actions.repeat(1,3).unsqueeze(2)\n",
    "result = torch.gather(res, dim=2, index=idx).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([222.0000,  10.1000], grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(result, torch.tensor([1,1,1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "tensor([[ 45.1000,  90.2000, 135.3000, 180.4000, 225.5000],\n",
      "        [  4.3500,   8.3000,  12.2500,  16.2000,  20.1500]])\n"
     ]
    }
   ],
   "source": [
    "# Choose max action \n",
    "q_all = res\n",
    "print(q_all.shape)\n",
    "\n",
    "w = torch.tensor([0.1,0.5,0.4])\n",
    "# We want (2,3,5) x (...) = (2,5)\n",
    "res2 = torch.matmul(w, q_all).detach()\n",
    "\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(res2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,  10, 100],\n",
       "        [  2,  20, 200],\n",
       "        [  4,  40, 400]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_acc = torch.tensor([1,2,4])\n",
    "r_div = torch.tensor([1,2,4])*10\n",
    "r_nov = torch.tensor([1,2,4])*100\n",
    "torch.stack([r_acc, r_div, r_nov], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0171, -0.0171, -0.0171, -0.0171,  0.0683])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Softmax\n",
    "a=torch.tensor([1.0,1,1,1,5],requires_grad=True)\n",
    "\n",
    "res = torch.nn.functional.softmax(a, dim=0)\n",
    "final = 10+torch.log(res[-1])\n",
    "final.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2289, -0.2289, -0.2289, -0.2289,  0.9158])\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "a=torch.tensor([1.0,1,1,1,0],requires_grad=True)\n",
    "\n",
    "res = torch.nn.functional.softmax(a, dim=0)\n",
    "final = 10+torch.log(res[-1])\n",
    "final.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2498, -0.2498, -0.2498, -0.2498,  0.9994])\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "a=torch.tensor([1.0,1,1,1,-5],requires_grad=True)\n",
    "\n",
    "res = torch.nn.functional.softmax(a, dim=0)\n",
    "final = 10+torch.log(res[-1])\n",
    "final.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecommenderModels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d88d4b03a2ecce5ef9fa6e2b4fba24fc59644f272c7aa6cbff04d59cd6dbfc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
